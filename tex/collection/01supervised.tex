% reset section counter

\setcounter{section}{0}

\metadata{1}{Anusri Pampari and Gabriel Poesia}{Jan 11th, 2021}


In this chapter, we will set up the standard theoretical formulation of supervised learning and introduce the \textit{empirical risk minimization} (ERM) paradigm. The setup will apply to almost the entire monograph \tnotelong{to be updated}and the ERM paradigm will be the main focus of Chapter~\ref{chap:asymp}, \ref{chap:conc}, and \ref{chap:uc}. 

\sec{Supervised learning}\label{lec1:sec:sup-learn}
In supervised learning, we have a dataset where each data point is associated with a label, and we aim to learn from the data a function that maps data points to their labels. The learned  function can be used to infer the labels of test data points. More formally, suppose the data points, also called inputs,  belong to some input space $\cX$ (e.g. images of birds), and labels belong to the output space $\cY$ (e.g. bird species). Suppose we are interested in a specific joint probability distribution $P$ over $\cX \times \cY$ (e.g. images of birds in North America), from which we draw a \emph{training set}, i.e we draw a a set of $n$ independent and identically distributed (i.i.d.) data points $\{(x^{(i)}, y^{(i)}\}_{i=1}^n$ from $P$. The goal of supervised learning is to learn a mapping (i.e. a function) from $\cX$ to $\cY$ using the training data. Any such function $h : \cX \rightarrow \cY$ is called a \emph{predictor} (also \emph{hypothesis} or \emph{model}).

Given two predictors, how do we decide which is better? For that, we define a \emph{loss function} over the predictions. There are several ways to define loss functions: for now, define a loss function $\ell$ as a function $\ell : \cY \times \cY \rightarrow \R$. Intuitively, the loss function takes two labels, the prediction made by a model $\hat{y}$ and the true label $y$, and gives a number that captures how different the two labels are. We assume $\ell$ is non-negative, i.e $\ell(\hat{y}, y) \geq 0$. Then, the loss of a model $h$ on an example $(x, y)$ is $\ell(h(x), y)$, i.e. the difference (as measured by $\ell$) between the prediction made by $h$ and the true label.


With these definitions, we are able to formalize the problem of supervised learning. Precisely, we seek to find a model $h$ that minimizes what we call the expected loss (or population loss or expected risk or population risk):
\al{
    L(h) \defeq{} \Exp_{(x, y)\sim p} [\ell(h(x), y)].
}


Note that $L$ is nonnegative because $\ell$ is nonnegative. Typically, the loss function is designed so that the best possible loss is zero when $\hat{y}$ matches $y$ exactly. Therefore, the goal is to find $h$ such that $L(h)$ is as close to zero as possible. % The best possible $h$ would have , we would find an $h$ with expected loss $0$, since that's the best possible we can do.

\paragraph{Examples: regression and classification problems.}

Here are two standard types of supervised learning problems based on the properties of the output space:

\begin{itemize}
    \item In the problem of \emph{regression}, predictions are real numbers ($\cY = \R$). We would like predictions to be as close as possible to the real labels. A classical loss function that captures this is the squared error, $\ell(\hat{y}, y) = (\hat{y} - y)^2$.
    \item In the problem of \emph{classification}, predictions are in a discrete set of $k$ unordered classes $\cY = [k] = \{1, \cdots, k \}$. One possible classification loss is the $0-1$ loss: $\ell(\hat{y}, y) = \mathbbm{1}(\hat{y} \neq y)$, i.e. $0$ if the prediction is equal to the true label, and $1$ otherwise.
\end{itemize}

\paragraph{Hypothesis class.}

So far, we said we would like to find \emph{any function} that minimizes population risk. However, in practice, we do not have a way of optimizing over arbitrary functions. Instead, we work within a more constrained set of functions $\cH$, which we call the \emph{hypothesis family} (or \emph{hypothesis class}). Each element of $\cH$ is a function $h : \cX \rightarrow \cY$. Usually, we choose a set $\cH$ that we know how to optimize over (e.g. linear functions, or neural networks).

Given one particular function $h \in \cH$, we define the \emph{excess risk} of $h$ with respect to $\cH$ as the difference between the population risk of $h$ and the best possible population risk inside $\cH$:

$$E(h) \defeq{} L(h) - \inf_{g\in\cH} L(g).$$

Generally we need more assumptions about a specific problem and hypothesis class to bound absolute population risk, hence we focus on bounding the excess risk.

Usually, the family we choose to work with can be parameterized by a vector of parameters $\theta \in \Theta$. In that case, we can refer to an element of $\cH$ by $h_\theta$, making that explicit. An example of such a parametrization of the hypothesis class is $\cH = \{ h: h_\theta(x) = \theta^\top x, \theta \in \mathbb{R}^d \}$.

\sec{Empirical risk minimization}

Our ultimate goal is to minimize population risk. However, in practice we do not have access to the entire population: we only have a \emph{training set} of $n$ data points, drawn from the same distribution as the entire population. While we cannot compute population risk, we can compute \emph{empirical risk}, the loss over the training set, and try to minimize that. This is, in short, the paradigm known as \emph{empirical risk minimization} (ERM): we optimize the training set loss, with the hope that this leads us to a model that has low
population loss. From now on, with some abuse of notation, we often write $\ell(h_\theta(x),y)$ as $\ell((x,y),\theta)$ and use the two notations interchangeably.  Formally, we define the empirical risk of a model $h$ as:
\al{
\hatL(h_\theta) \defeq{} \frac{1}{n} \sum_{i=1}^n \ell(h_\theta(x^{(i)}), y^{(i)}) = \frac{1}{n} \sum_{i=1}^n \ell((x^{(i)}, y^{(i)}), \theta).
}
\emph{Empirical risk minimization} is the method of finding the minimizer of $\hatL$, which we call $\hat{\theta}$:
\al{
    \label{lec1:eqn:erm}
    \hat{\theta} \defeq{} \argmin_{\theta\in\Theta} \hatL(h_\theta).
}
Since we are assuming that our training examples are drawn from the same distribution as the whole population, we know that empirical risk and population risk are equal
\emph{in expectation} (over the randomness of the training dataset):
\al{
    \Exp_{(x^{(i)}, y^{(i)}) \iid P}\ \hatL(h_\theta) &= \Exp_{(x^{(i)}, y^{(i)}) \iid P} \frac{1}{n} \sum_{i=1}^n \ell(h_\theta(x^{(i)}), y^{(i)}) \\
    &= \frac{1}{n} \sum_{i=1}^n \Exp_{(x^{(i)}, y^{(i)}) \iid P} \ell(h_\theta(x^{(i)}), y^{(i)}) \\
    &= \frac{1}{n} \cdot{} n \cdot{} \Exp_{(x^{(i)}, y^{(i)}) \iid P} \ell(h_\theta(x^{(i)}), y^{(i)}) \\
    &= L(h_\theta).
}


This is one reason why it makes sense to use empirical risk: it is an unbiased estimator of the population risk.

The key question that we seek to answer in the first part of this course is: \textbf{what guarantees do we have on the excess risk for the parameters learned by ERM?} The hope with ERM is that minimizing the training error will lead to small testing error. One way to make this rigorous is by showing that the ERM minimizer's excess risk is bounded.
