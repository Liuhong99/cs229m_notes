\sec{Deep neural nets (via covering number)}\label{sec:deep_nets}
In Section~\ref{lec9:sec:cover_to_radem}, we discuss how strong our bounds on covering number need to be in order to get a useful result. 
Here we describe some situations in which we know how to obtain these covering number bounds for concrete models such as linear models and neural networks. 

\subsec{Preparation: covering number for linear models}
First, consider the following covering number bound for linear models:

\begin{theorem}[\cite{zhang2002}] \label{lec9:thm:univariate_rad}
Suppose $x^{(1)}, \cdots, x^{(n)} \in \mathbb{R}^d$ are $n$ data points, and $p, q$ satisfies $1/p + 1/q = 1$ and $2 \le p \le \infty$. Assume that $||x^{(i)}||_p \le C$ for all $i$. Let:
\begin{align}
    \cF_q = \{x \mapsto w^\top x : ||w||_q \le B\}
\end{align}
and let $\rho = L_2(P_n)$. Then, $\log N(\epsilon, \cF_q, \rho) \le \l [\frac{B^2C^2}{\epsilon^2}\r ] \log_2 (2d + 1)$. When $p = 2, q = 2$, we further obtain that:
\begin{align}
    \log N(\epsilon, \cF_2, \rho) \le \l [\frac{B^2C^2}{\epsilon^2} \r ] \log_2 (2 \min (n, d ) + 1)
\end{align}
\end{theorem}
\begin{remark}
Applying \eqref{lec9:eqn:rademacherbound_three} to the covering number bound derived above with $R = B^2C^2$, we conclude that the Rademacher complexity of this class of linear models satisfies
\begin{align}
    R_S(\cF_q) &\le \tilO{\left( \frac{BC}{\sqrt{n}} \right)}.
\end{align} 
We also prove this result without relying on Dudley's theorem in Theorem~\ref{lec7:thm:l2-thm}.
\end{remark}
Next, we consider multivariate linear functions as they are building blocks for multi-layer neural networks. Let $M = (M_1, \cdots, M_n) \in \mathbb{R}^{m \times n}$ and $\norm{M}_{2,1} = \sum_{i = 1}^n \norm{M_i}_2$. Then, $\norm{M^\top}_{2,1}$ denotes the sum of the $\ell_2$ norms of the rows of $M$. 
\begin{theorem}\label{lec9:thm:multivariate_rad}
Let $\cF = \{x \to Wx : W \in \mathbb{R}^{m \times d}, ||W^\top||_{2, 1} \le B\}$ and let $C = \sqrt{\frac{1}{n} \sum_{i = 1}^n ||x^{(i)}||_2^2}$. Then, 
\begin{equation}
\log N(\epsilon, \cF, L_2(P_n)) \le \l [\frac{c^2B^2}{\epsilon^2} \r ] \ln (2dm).
\end{equation}
\end{theorem}
\begin{remark}
    In some sense, Theorem~\ref{lec9:thm:multivariate_rad} arises from treating each dimension of the multivariate problem independently. We can view the linear layer as applying $m$ different linear functions. Explicitly, if $W = \begin{pmatrix} w_1^\top \\ \vdots \\ w_m^\top \end{pmatrix}$ and $Wx = \begin{pmatrix} w_1^\top x \\ \vdots \\ w_m^\top x \end{pmatrix}$, then as we expect, $\norm{W^\top}_{2,1} = \sum \norm{w_i}_2$.
\end{remark}


\subsec{Deep neural networks}
In this lecture, we discuss a bound on the Rademacher complexity of a dense neural network. We set up notation as follows: $W_i$ denotes the linear weight matrix at the $i$-th layer of the neural network, we have a total of $r$ layers, and $\sigma$ is the activation function which is 1-Lipschitz (for example, ReLU, softmax, or sigmoid). If the input is a vector $x$, the neural network's output can be represented as follows:

\begin{align}
    f(x) = W_r\sigma(W_{r-1}\sigma(\cdots \sigma(W_1x)\ldots),
\end{align}
Using this notation, we establish an upper bound on the Rademacher complexity of a dense neural network.

\begin{theorem}[\cite{bartlett2017}]
\label{lec10:thm:dnn_rademacher}
Suppose that $\forall i, \norm{x^{(i)}}_2 \leq c$ and let
\begin{align}
    \cF = \{h_\theta : \norm{W_i}_{\text{op}} \leq \kappa_i, \norm{W_i^T}_{2,1} \leq b_i\}.
\end{align}
Then,
\begin{equation}
    R_S (\cF) \leq \frac{c}{\sqrt{n}} \cdot \underbrace{\left(\prod_{i=1}^r \kappa_i \right)}_{\text{relatively large}} \cdot \underbrace{\left( \sum_{i=1}^r\frac{b_i^{2/3}}{\kappa_i^{2/3}}\right)^{3/2}}_{\text{relatively small}}.
\end{equation}
\end{theorem}

\begin{remark}
    We use $\norm{W}_{\textup{op}}$ to denote the operator norm (or spectral norm) of $W$, and $\norm{W_i^\top}_{2,1}$ denotes the sum of the $l_2$ norms of the rows of $W_i$. We note that $f(x) = Wx$ is Lipschitz with a Lipschitz constant of $\norm{W}_{\textup{op}}$. This is because $\norm{f(x)-f(y)}_2 = \norm{Wx-Wy}_2 \leq \norm{W}_{\textup{op}}\norm{x-y}_2$, since $\norm{W}_{\textup{op}} = \max_{x:\norm{x}_2=1}\norm{Wx}_2$. The second term is relatively small as it is a sum of matrix norms, and so the bound is dominated by the first term, which is a product of matrix norms.
\end{remark}

\begin{remark}
    As a corollary of the above theorem, we also get a bound on the generalization error for the margin loss of the following form:
    \begin{equation}
        \texttt{generalization error} \leq \tilde{O}\left(\frac{1}{\gamma_{\min}} \cdot \frac{1}{\sqrt{n}}\cdot c \cdot \left(\prod_{i=1}^r\norm{W_i}_{\textup{op}} \right) \cdot {\left( \sum_{i=1}^r\frac{\norm{W_i^\top}^{2/3}_{2,1}}{\norm{W_i}_{\textup{op}}^{2/3}}\right)^{3/2}}  \right),
    \end{equation}
    where $\gamma_{\min}$ denotes the margin.
\end{remark}
	
First, we motivate the proof by presenting the main idea, and then work through each part of the proof. The main ideas of the proof can be summarized as follows:
    
\begin{itemize}
    \item At a high level, we want to show that the covering number $N(\epsilon, \cF, \rho)$ for a dense neural network is $\leq \frac{R}{\epsilon^2}$. Proving this would enable us to apply the Localized Dudley Theorem to get a bound on the Rademacher Complexity.
    \item To bound the covering number for a dense neural network, we use $\epsilon$-covers to cover each layer of $f$ separately, and then combine them to prove that there exists an $\epsilon$-cover of the original function $f$. 
    \item To combine the epsilon covers of each layer, we use the Lipschitzness of each layer.
    \item We control and approximate the error propagation that is introduced through discretizing each layer using $\epsilon_i$-coverings in order to get a reasonable final $\epsilon$.
\end{itemize}

As a prelude to the proof, let us think of each layer of $f$ as a separate function $f_i$. Note that $f_i$ corresponds to a matrix multiplication and an activation layer. Thus, $f$ can be written as 
\begin{align}
    f = f_r \circ f_{r-1} \circ \cdots \circ f_1,
\end{align}
where each $f_i$ is $\kappa_i$-Lipschitz. Let us also assume, for simplicity, that $f_i(0) = 0$. 

We now derive the $\epsilon$-covering of $f(x)$ in three steps:
\begin{enumerate}
    \item We take points as input to the $i^{th}$ layer, and devise an $\epsilon_i$ covering of the space spanned by each point passed through the function $f_i$.
    \item We use the $N(\epsilon_i, \cF_i, \rho)$ points of the $\epsilon_i$ covering as an input to the $(i+1)$-th layer, where $\cF_i$ is the function family of the $i$-th layer and $\rho = L_2(p_n)$.
    \item Finally, the $\epsilon_r$ covering (i.e covering of the last layer) will be the $\epsilon$ covering of $f(x)$.
\end{enumerate}

Before stating a critical lemma, we establish some definitions. First, define $c_i$ such that all the inputs to the $i^{th}$ layer, $z_1, z_2, \dots, z_n$ are bounded as $\norm{z_j} \leq c_{i-1}$, and therefore $c_0 = c$. We also assume that there exists a function $g$ such that $\log N (\epsilon_i, \cF_i, \rho) \leq g\left(\epsilon_i, c_{i-1}\right)$. Then, we can state the following lemma:

\begin{lemma}
    There exists an $\epsilon$ cover of $\mathcal{F} = \{f(x) : f(x) = f_r \circ f_{r-1} \circ \cdots \circ f_1\}$, where $\epsilon = \epsilon_r + \kappa_r\epsilon_{r-1} + \cdots + \kappa_r\kappa_{r-1}\dots\kappa_2\epsilon_1$, and
    \begin{align}
    \log N(\epsilon, \cF, \rho) \leq \sum_{i=1}^{r} g\left(\epsilon_i, c_{i-1}\right)
    \end{align}
    \label{lec10:lma:additive_cover}
\end{lemma}

\begin{proof}
We define $\cC_1$ to be an $\epsilon_1$-cover of $\cF_1$. For all $f_1' \in \cC_1$, construct $\cC_{2, f_1'}$ to $\epsilon_2$ cover the set $\cF_2 \circ f_1' = \left\{f_2\left(f_1'\left(X\right)\right) : f_2 \in \cF_{2,f_1'} \right\}$. Then, we define $\cC_2 = \cup_{f_1'\in \cC_1}\cC_{2,f_1'}$, and observe that $\cC_2$ a cover of $\cF_2 \circ \cF_1$ with $\epsilon = \epsilon_1 \times \kappa_2 + \epsilon_2$.

We now note that
\begin{align}
    \log \abs{\cC_{2, f_1'}} \leq g\left(\epsilon_2, c_1\right)
\end{align}
Taking a union over all $f_1'$, 
\begin{align}
    \abs{\cC_{2}} &\leq \abs{\cC_{1}} \exp\left(g\left(\epsilon_2, c_1\right)\right) \\
    \log\abs{\cC_{2}} &\leq \log\abs{\cC_{1}} + g\left(\epsilon_2, c_1\right) \leq g\left(\epsilon_1, c_0\right) + g\left(\epsilon_2, c_1\right)
\end{align}
Similarly, given $\cC_k$, for any $f_k' \circ f_{k-1}' \circ \cdots \circ f_1' \in \cC_k$, we construct a $\cC_{k+1, f_k', \dots, f_1'}$ that is an $\epsilon_{k+1}$-covering of $\cF_{k+1} \circ f_k' \circ \cdots \circ f_1'$. We choose $\cC_{k+1} = \cup_{f_i \in \cC_i, i \leq k} C_{k+1, f_k', \dots, f_1'}$. Then, we have
\begin{align}
    \log \abs{\cC_{k+1}} \leq g\left(\epsilon_{k+1}, c_k\right) + \cdots + g\left(\epsilon_1, c_0\right)
\end{align}
Next, we show that for the above construction, the radius of the cover for $\cF$ is
\begin{align}
    \epsilon = \sum_{i=1}^{r} \left(\epsilon_i \prod_{j=i+1}^{r}\kappa_{j}\right).
\end{align}
If we consider a function composition $f_r \circ \cdots \circ f_1 \in \cF_r \circ \cF_{r-1} \circ \cdots \circ \cF_1$, then we know that there exists an $f_1' \in \cC_1$ such that $\rho\left(f_1, f_1'\right) \leq \epsilon_1$. Similarly, we know there exists $f_{2, f_1'}' \in \cC_{2, f_1'}$ such that $\rho\left(f_2' \circ f_1', f_2\circ f_1' \right) \leq \epsilon_2$. Using the triangle inequality, we can say that 
\begin{align}
   \rho\left(f_2' \circ f_1', f_2 \circ f_1\right) &\leq \rho\left(f_2' \circ f_1', f_2 \circ f_1'\right) + \rho\left(f_2 \circ f_1', f_2 \circ f_1\right) \\ 
   &\leq \epsilon_2 + \rho\left(f_2 \circ f_1', f_2 \circ f_1\right) \\ 
   &\leq \epsilon_2 + \kappa_2 \rho\left(f_1', f_1\right) \\ 
   &\leq \epsilon_2 + \kappa_2\epsilon_1z
\end{align}
Using the same logic for a general $k$, we know that there exists $f_{k,f_1',\dots,f_{k-1}'}' \in \cC_k$ such that
\begin{align}
    \rho\left(f_k' \circ f_{k-1}' \circ \cdots \circ f_1', f_k \circ \cdots \circ f_1\right) &\leq \rho\left(f_k' \circ f_{k-1}'\circ \cdots \circ f_1', f_k \circ f_{k-1}'\circ \cdots \circ f_1' \right) \\ 
    &+ \rho\left(f_k \circ f_{k-1}'\circ f_{k-2}' \circ \cdots \circ f_1', f_k \circ f_{k-1}\circ f_{k-2}' \circ \cdots \circ f_1'\right) \\ &+ \cdots + \rho\left(f_k \circ f_{k-1}\circ \cdots \circ f_2 \circ f_1', f_k \circ f_{k-1}\circ \cdots \circ f_1\right) \\ & \leq \sum_{i=1}^{r} \left(\epsilon_i\prod_{j=i+1}^{r}\kappa_{j}\right)
\end{align}
\end{proof}

\begin{proof}[Proof of Theorem~\ref{lec10:thm:dnn_rademacher}]
We now apply Lemma~\ref{lec10:lma:additive_cover} to dense neural networks. Dense neural networks consist of a composition of layers, where each layer is a linear model composed with a 1-Lipschitz activation. Using Theorem~\ref{lec9:thm:multivariate_rad} along with the property that 1-Lipschitz functions will only contribute a factor of at most $1$ (Lemma~\ref{lec9:lma:talagrand}), the covering number of each layer can be bounded by:
\begin{align}
    g\left(\epsilon_i, c_{i-1}\right) = \tilde{O}\left(\frac{c_{i-1}^2b_i^2}{\epsilon_i^2}\right),
\end{align}
where $c_{i-1}^2$ is the norm of the inputs, $b_i^2$ is $\norm{W_i^T}_{2,1}$, and $\epsilon_i^2$ is the radius. From Lemma~\ref{lec10:lma:additive_cover}, we know that 
\begin{align}
    \log N(\epsilon, \cF, \rho) &= \tilde{O}\left(\sum_{i=1}^{r}\frac{c_{i-1}^2b_i^2}{\epsilon_i^2}\right) 
\end{align}
for
\begin{align}
    \epsilon &= \sum_{i=1}^{r} \left(\epsilon_i \prod_{j=i+1}^{r}\kappa_j\right)
\end{align}

We now have a bound on $N(\epsilon, \cF, \rho)$ that relies on $\epsilon_i$'s, but $N(\epsilon, \cF, \rho)$ should only be a function of $\epsilon$. Since we already know that $\epsilon = \sum_{i=1}^{r} \left(\epsilon_i \prod_{j=i+1}^{r}\kappa_j\right)$, we keep $\epsilon$ constant and optimize the upper bound of $N(\epsilon, \cF, \rho)$ over different choices of $\epsilon_i$. To find the optimal $\epsilon_i$, we will first find a lower bound on $N(\epsilon, \cF, \rho)$. We then choose $\epsilon_i$ so that this lower bound is achieved. Ultimately, our optimized $\epsilon_i$ yields a bound on the covering number of the following form: $\log\left(N\left(\epsilon, \cF, \rho\right)\right) \leq \frac{R}{\epsilon^2}$, where $R$ is some constant independent of $\epsilon$. 

We derive this lower bound using Holder's inequality, which states that
\begin{align}
    \langle a,  b \rangle \leq \|a\|_p \|b\|_q
\end{align}
when $\frac{1}{p} + \frac{1}{q} = 1$. Writing out the vectors $a, b$, we get that 
\begin{align}
    \sum_{i}a_ib_i \leq \left(\sum a_i^p\right)^{\frac{1}{p}}\left(\sum b_i^q\right)^{\frac{1}{q}}
\end{align}

Letting $\alpha_i^2 = c_{i-1}^2b_i^2, \beta_i = \prod_{j=i+1}^{r}\kappa_j$. By Holder's inequality, using $p = 3, q = \frac{3}{2}$, we get
\begin{align}
    \left(\sum_{i=1}^{r}\frac{\alpha_i^2}{\epsilon_i^2}\right)\left(\sum_{i=1}^{r}\beta_i\epsilon_i\right)^2 &\geq \left(\sum_{i=1}^{r}\left(\alpha_i\beta_i\right)^{\frac{2}{3}}\right)^{\frac{3}{2}}
\end{align}
\begin{align}
    \sum_{i=1}^{r}\frac{\alpha_i^2}{\epsilon_i^2} &\geq \frac{R}{\epsilon^2},
\end{align}
where $R = \left(\left(\sum_{i=1}^{r}\left(c_{i-1}b_i\prod_{j=i+1}^{r}\kappa_j\right)\right)^{\frac{2}{3}}\right)^{\frac{3}{2}}$. We note that equality holds when 
\begin{align}
    \epsilon_i = \left(\frac{c_{i-1}^2b_i^2}{\prod_{j=i+1}^{r}\kappa_j}\right)^{\frac{1}{3}} \cdot \underbrace{\frac{\epsilon}{\left(\sum_{i=1}^{r}\frac{b_i^{\frac{2}{3}}}{\kappa_i^{\frac{2}{3}}}\right)\prod_{i=1}^{r}\kappa_i^{\frac{2}{3}} }}_{\epsilon'} \label{eqn:lec10:holder_eps_defn}
\end{align}
Using this choice of $\epsilon_i$ and letting $\epsilon'$ equal the second factor in \eqref{eqn:lec10:holder_eps_defn} for notational convenience, we know that the log covering number is (up to a constant factor):
\al{
    \sum_{i=1}^r \frac{c_{i-1}^2b_i^2}{\epsilon_i^2} &= \sum_{i=1}^r \frac{c_{i-1}^2b_i^2(\kappa_{i+1}\cdots\kappa_r)^\frac{2}{3}}{c_{i-1}^\frac{4}{3}b_i^\frac{4}{3}(\epsilon')^2} \\
    &= \sum_{i=1}^r (c_{i-1}b_i\kappa_{i+1}\cdots\kappa_r)^\frac{2}{3}\frac{1}{(\epsilon')^2} \\
    &= c^\frac{2}{3}\sum_{i=1}^r \left(\frac{b_i}{\kappa_i}\right)^\frac{2}{3} \prod_{i=1}^r \kappa_i^\frac{2}{3} \frac{\left(c^\frac{2}{3}\left(\sum_{i=1}^r (\frac{b_i}{\kappa_i})^\frac{2}{3} \prod_{i=1}^r \kappa_i^\frac{2}{3}\right)\right)^2}{\epsilon^2} \\
    &= \left(c^\frac{2}{3}\sum_{i=1}^r \left(\frac{b_i}{\kappa_i}\right)^\frac{2}{3} \prod_{i=1}^r \kappa_i^\frac{2}{3}\right)^3\frac{1}{\epsilon^2} \\
    &= c^2\prod_{i=1}^r \kappa_i^2\left(\sum_{i=1}^r \left(\frac{b_i}{\kappa_i}\right)^\frac{2}{3}\right)^3\frac{1}{\epsilon^2}.
}
Since this log covering number is of the form $R / \epsilon^2$, we can apply \eqref{lec9:eqn:rademacherbound_three} and conclude that
\al{
    \mathcal{R}_S(\cF) \lesssim \sqrt\frac{R}{n}
}
Last, plugging in
\al{
    R = c^2\prod_{i=1}^r \kappa_i^2\left(\sum_{i=1}^r \left(\frac{b_i}{\kappa_i}\right)^\frac{2}{3}\right)^3
}
we obtain the desired result
\al{
    \mathcal{R}_S(\cF) \lesssim \frac{c}{\sqrt n}\prod_{i=1}^r \kappa_i\left(\sum_{i=1}^r \left(\frac{b_i}{\kappa_i}\right)^\frac{2}{3}\right)^\frac{3}{2}.
}

\end{proof}

\chapter{Data-dependent Generalization Bounds for Deep Nets}\label{sec:deep_nets_data_dependent}

In Theorem~\ref{lec10:thm:dnn_rademacher}, we proved the following bound on the Rademacher complexity of deep neural networks:
\begin{align}
    R_S(\cF) \leq \prod_{i = 1}^r \norm{W_i}_{\text{op}} \cdot \mathsf{poly}(\norm{W_i}).
\end{align}
This bound, however, suffers from multiple deficiencies. In particular, it grows exponentially in the depth, $r$, of the network and $\norm{W_i}_{\text{op}}$ measures the worst-case Lipschitz-ness of the network layers over the input space. As a consequence, the bound fails to accurately predict the good generalization properties of deep nets.

In this section, we obtain a tighter bound for $R_S(\cF)$ by modifying our approach to depend upon the realized Lipschitz-ness of the model on the training data. To further motivate this approach, we also note that stochastic gradient descent, i.e. the typical optimization method typically used to fit deep neural networks, prefers models that are more Lipschitz. This preference must be realized by the model \emph{on empirical data}, however, as no learning algorithm has access to the model's properties over the entire data space.

Ultimately, we aim to prove a tighter bound on the population loss that grows polynomially in the Lipschitz-ness of $f \in \cF$. Namely, given that $f_\theta$ is $\kappa$-Lipschitz on $X^{(1)},\dots,X^{(n)}$, we hope to show that:
\begin{align}
    L(\theta) \leq \mathsf{poly} (\kappa, \norm{\theta}).
\end{align}
\tnote{something wrong with the equation}

\paragraph{Uniform convergence with a data-dependent hypothesis class.}
Classical uniform convergence does not have a single consistent definition. So far in this course, given some complexity measure we denote as $\text{comp}(\cdot)$, our uniform convergence results always appear in one of the two following forms: 
\tnote{let's move whp to up front because this phrasing makes the order of the quantifier a bit tricky}
\begin{align}
    L(f) &\leq \frac{\text{comp}(\cF)}{\sqrt{n}} \text{ (w.h.p.) $\forall f \in \cF$} &&\text{(I)} \\
    L(f) &\leq \frac{\text{comp}(f)}{\sqrt{n}} \text{ (w.h.p.) $\forall f \in \cF$} &&\text{(II)}
\end{align}

\begin{remark}
    Most of the results we have obtained so far are of type I, e.g. $\text{comp}(\cF)$ is a Rademacher complexity. We obtain results of type II by considering a restricted set of functions $\cF_C = \{f : \text{comp}(f) \leq C\}$. We then apply a type I bound to $\cF_C$ and take a union bound over all $C$.
\end{remark}

Note, however, that neither of these approaches produce bounds that depend upon the data. By contrast, in the sequel, we will derive a new \textit{data-dependent} generalization bound. These bounds state that with high probability over the choice of the empirical data and for all functions $f$, 
\begin{align}
    L(f) \leq \text{comp}\left (f, \{x\sp{i}, y\sp{i}\}_{i = 1}^n\right)
\end{align}
A further advantage of this approach is that this bound can be used as a regularizer.

\begin{remark}
Although there is no universal consensus on the type of generalization bound we should derive, we can argue that there is no way to improve upon this bound. For example, one might try to use the input distribution $P$ to define our complexity measure, but if we allowed ourselves access to $P$, we could just define $\text{comp}(f, P) = \Exp_P[f(X)]$. In some sense, defining a generalization bound using the true distribution amounts to cheating, so it becomes difficult to define a distibution-dependent generalization bound in a principled way.
\end{remark}

In this new paradigm, we can no longer reduce bounds of type I to bounds of type II using a union bound. To see why, suppose that we have the hypothesis class
\begin{align}
    \cF = \{f: \text{comp}(f, \{x\sp{i}, y\sp{i}\}_{i=1}^n) \leq C)\}
\end{align}
If our complexity measure depends on the empirical data, then so does our hypothesis class, which makes $\cF$ itself a random variable. However, our theorems regarding Rademacher complexity require that the hypothesis class be fixed before we ever see the empirical data.

We get around this by changing the way we think about uniform convergence. Suppose that our new complexity measure is separable, i.e.
\begin{align}
    \text{comp}(f, \{x\sp{i}, y\sp{i}\}_{i=1}^n) = \sum_{i=1}^n g(f, x\sp{i}),
\end{align}
for some function $g$. Then we can consider an \textit{augmented loss}:
\begin{align}
    \tilde{\ell}(f) = \ell(f) \mathbf{1}(g(f, x\sp{i} \leq C))
\end{align}
Suppose we have a region of low complexity in our existing loss function. Because this region is random, so we cannot selectively apply universal convergence. However, we can use our new surrogate loss function $\tilde{\ell}$ in that region. By modifying the loss function in this way, we can still fix the hypothesis class ahead of time, allowing us to apply existing tools to $\tilde{\ell}(f)$. In the sequel, we introduce a particular surrogate ``margin'' that allows us to cleanly apply our previous results to a implicitly data-dependent hypothesis class \cite{wei2019data}.

\sec{All-layer margin}
We next introduce a new surrogate loss called the \textit{all-layer margin} that can also be thought of as a surrogate margin. This loss will essentially zero out high-complexity regions so that we may focus on low-complexity regions. Once we adopt this new loss function, we will be able to apply some of our earlier methods.

Let $f: \R^d \to \R$ be a classification model. Recall that the standard margin is defined as $y f(x)$, with $y$ in $\{-1, 1\}$. We will say that $g_f(x, y)$ is a \textit{generalized margin} if it satisfies
\begin{align}
    g_f(x, y) = \begin{cases}
0,& \text{ if } f(x)y \leq 0 \text{ (an incorrect classification)}\\
> 0,& \text{ if } f(x)y > 0 \text{ (a correct classification)}
\end{cases}.
\end{align}
That is, the generalized margin ``zeroes out" incorrect classifications.

We also define the \textit{$\infty$-covering number} $N_\infty(\epsilon, \cF)$ as the minimum cover size with respect to the metric $\rho = \norm{\cdot}_\infty$, Precisely,
\begin{equation}
\rho(f, f) = \sup_{x \in \mathcal{X}} |f(x) - f'(x)| = \|f - f'\|_\infty
\end{equation}
\begin{remark}
    Notice that $N_\infty(\epsilon, \cF) \geq N(\epsilon, \cF, L_2(P_n))$. This is because the $\ell_\infty$ norm is a more demanding measure of error: $f$ and $f'$ must be close on \textit{every} input, not just the empirical data. That is,
    \begin{equation}
    \sqrt{\frac{1}{n} \sum_{i=1}^n (f(x_i) - f'(x_i))^2} \leq \sup_{x \in \mathcal{X}} |f(x) - f'(x)|.
    \end{equation}
\end{remark}

\begin{lemma}
Suppose $g_f$ is a generalized margin. Let $\cG = \{g_f: f \in \mathcal{F}\}$. Suppose that for some $R$, $\log N_\infty(\epsilon, \cG) \leq \lfloor \frac{R^2}{\epsilon^2} \rfloor$ for all $\epsilon > 0$. (Recall that this is the worst regime that we can tolerate when working with Rademacher complexity.) Then, with probability greater than or equal to $1 - \delta$ over the randomness in the training data, for all $f$ in $\mathcal{F}$ that correctly predict all the examples,
\begin{equation}
L_{0/1} \leq \tilO \l (\frac{1}{\sqrt{n}} \cdot \frac{R}{\min g_f(x\sp{i}, y\sp{i})} \r ) + \tilO\l (\frac{1}{\sqrt{n}}\r ).
\end{equation}
\label{lec11:genmargin-lemma}
\end{lemma}

\begin{proof}
The high-level idea of our proof is to replace $\cF$ with $\cG$ before repeating a standard margin theory argument.

Let $\ell_\gamma$ be the ramp loss, which is 1 for negative values, 0 for values greater than $\gamma$, and a linear interpolation between 1 and 0 for values between 0 and $\gamma$. We define the surrogate loss as $\hat{L_\gamma(\theta)} = \frac{1}{n} \ell_\gamma(g_{f_\theta}(x\sp{i}, y\sp{i}))$, and the surrogate population loss as $\Exp[\ell_\gamma(g_{f_\theta}(x, y))]$. As before, when we used the Rademacher complexity to control the difference between these two functions, we have that
\begin{equation}
L_\gamma(\theta) - \hat{L}_\gamma(\theta) \leq R_S(\ell_\gamma \circ \cG) + \tilO\l (\frac{1}{\sqrt{n}}\r ).
\end{equation}
Next we observe that $\log N(\epsilon, \ell_\gamma \circ \cG, L_2(P_n)) \leq \log N(\epsilon\gamma, G, L_2(P_n))$; this is due to the $\frac{1}{\gamma}$-Lipschitzness of $\ell_\gamma$. The right hand side is also less than $\log N_\infty(\epsilon\gamma, G)$, which is in turn bounded above by $\lfloor \frac{R^2}{\epsilon^2 \gamma^2} \rfloor$ by assumption.
Then, using our results relating the log of the covering number to a bound on the Rademacher complexity (recall \ref{lec9:eqn:rademacherbound_three} and Theorem~\ref{lec9:thm:better-dudley}), we have that $R_S(\ell_\gamma \circ G) \leq \tilO(\frac{R}{\gamma \sqrt{n}})$.
Take $\gamma = \gamma_{\min} = \min_{i} g_\gamma(x\sp{i}, y\sp{i})$.\footnote{A caveat: because $\gamma$ is a random variable, proving this result rigorously requires taking a union bound over a discretized $\gamma$. We sketched out this argument more thoroughly in Remark~\ref{lec7:rmk:union_bound_margin}.} Then we have $\hat{L}_{\gamma_\text{min}} (\theta) \leq 0 + \tilO(\frac{R}{\sqrt{n} \cdot \gamma_\text{min}}) + \tilO(\frac{1}{\sqrt{n}})$, as desired.
\end{proof}
For which $g_f$ can we bound the covering number? If we take $g_f(x, y) = yf(x)$, then the covering number depends on the product $\norm{w_i}_{\text{op}}$, but we originally set out to do better than this. If we have a linear model $w^T x$, the normalized margin, $\frac{y \cdot w^T x}{\norm{w}}$, governs the generalization performance. But how do we normalize for more general models? 

For a deep neural net, a natural normalizer is the product of the Lipschitz constants of the layers. However, we do not want to normalize by a constant that depends only on the function class, so we take a different approach. We interpret the normalized margin as the minimum $\delta$ such that $w(x + \delta y) < 0$. In plain English, how can we find the minimum perturbation that gets our data point across the boundary?

To define this for all layers simultaneously, we define the \textit{all-layer margin}. We will consider a model in which we perturb all of the layers, not just the input (since perturbing that alone turns out not to suffice). We will consider perturbed models $\delta = (\delta_1, ..., \delta_r)$, where each $\delta_i$ is a vector. We incorporate these perturbations into our model as follows:
\begin{align}
    h_1(x, \delta) &= W_1 x + \delta_1 \cdot \norm{x}_2 \\
    h_2(x, \delta) &= \sigma(W_2 \cdot h_1(x, \delta)) + \delta_2 \cdot \norm{h_1(x, \delta)}_2 \\
    &\vdots \\
    f(x, \delta) = h_r(x, \delta) &= \sigma(W_r h_{r - 1}(x, \delta)) + \delta_r \cdot \norm{h_{r - 1}(x, \delta)}_2.
\end{align}
We can then ask: what was the smallest perturbation that changed our decision? That is, let
\begin{align}
    m_f(x, y) \defeq \min_\delta \sqrt{\sum_{i=1}^r ||\delta_i||_2^2} 
\end{align}
such that $f(x, \delta)y \leq 0$ (i.e., obtain incorrect predictions).

Informally, $m_f(x, y)$ is a measure of how hard it is to perturb the model $f$. $f$ can be hard to perturb for two reasons: $f$ is Lipschitz (in its intermediate layers), or $yf(x)$ is large. Even more informally, large margins imply confidence in our predictions, and so it becomes harder to change the model's mind.

We can now introduce our main result regarding the all-layer margin.
\begin{theorem}
With high probability,
\begin{equation}
L_{01}(f) \leq \tilO\l (\frac{1}{\sqrt{n}} \cdot \frac{\sum_{i=1}^n \norm{W_i}_{1, 1}}{\min_i} m_f(x\sp{i}, y\sp{i}\r ) + \tilO\l (\frac{r}{\sqrt{n}}\r ),
\end{equation}
where
$\norm{W_i}_{1, 1}$ is the sum of absolute values of entries of W.\footnote{It is not clear that this is the best bound we can hope for.}
\end{theorem}
In summary, robustness to perturbations in intermediate errors implies good generalization. We can show that this bound is strictly better than the ones we obtained previously; we have $\frac{1}{m_f(x, y)} \leq \frac{\prod \norm{W_i}_{\text{op}}}{f(x)}$.

To prove this theorem, it suffices to bound $N_\infty(\epsilon, \cG)$ by $O(\frac{\sum{||W_i||_{1, 1}}}{\epsilon^2})$. Then we can apply Lemma~\ref{lec11:genmargin-lemma}.

Let $\cF_i = \{ z \mapsto \sigma (W_i z) : ||W_i||_{1, 1} \leq \beta_i \}$. Then $\cF = \cF_r \circ \cF_{r-1} \circ \cdots \circ \cF_1$. Let $m \circ \cF$ denote $\{m_f : f \in \mathcal{F} \}$. Then
\begin{equation}
\log N_\infty\l (\sqrt{\sum_{i=1}^n \epsilon_i^2}, m \circ \cF \r ) \leq \sum_{i=1}^r \log N_\infty(\epsilon_i, \cF_i).
\end{equation}
where $N_\infty(\epsilon_i, \cF_i)$ is defined with respect to the input domain $\mathcal{X} = \{x : \norm{x}_2 \leq 1 \}$.

That is, we only have to find the covering number for each layer, and then we have the covering number for the (all-layer margin of the) composed function class. Notice the key difference: we used $m \circ \cF$ in the above, not $\cF$.

Then, the desired result follows assuming that the following ``decomposition lemma'' holds:

\begin{lemma}
    If $\log N_\infty(\epsilon_i, \cF_i) \leq \lfloor \frac{c_i^2}{\epsilon_i^2} \rfloor$, then if we take $\epsilon_i = \epsilon \cdot \frac{c_i}{\sqrt{\sum_i c_i^2}}$, we obtain:
    \begin{equation}
    \log N_\infty(\epsilon, m \circ \cF) \leq \frac{\sqrt{\sum_i c_i^2}}{\epsilon^2}.
    \end{equation}
\end{lemma}
This result gives the complexity of the composed model in terms of the complexity of the layers, with each $c_i$ given by $\norm{W_i}_{1, 1}$.

For linear models, we can show $N_\infty(\epsilon_i, \cF_i) \leq \tilO\l (\frac{\beta_i^2}{\epsilon^2} \r )$ (where $\beta_i$ is a bound on $\norm{W_i}_{1, 1}$), and this implies our main theorem.

\begin{proof}
Now we will prove a limited form of the decomposition lemma for affine models: $\cF_i = \{ z \mapsto \sigma(w_i z): \norm{w_i}_{1, 1} \leq \beta_i \}$. There are two crucial steps to this problem. First, we prove that $m_f(x, y)$ is 1-Lipschitz in $f$. That is, for all $\cF = \cF_r \circ \cF_{r-1} ... \circ \cF_1$ and $\cF' = \cF_r' \circ 
cF_{r-1}' ... \circ \cF_1'$, $|m_f(x, y) - m_{f'}(x, y)| \leq \sqrt{\sum_{i=1}^r \max_{\norm{x}_2 \leq 1} \norm{f_i(x) - f_i'(x)}_2}$. Notice that now we are working with a clean sum of differences, with no multipliers!

Second, we construct a cover: Let $U_1, \dots, U_r$ be $\epsilon_1, \dots, \epsilon_r$ covers of $\cF_1, ..., \cF_r$, respectively, such that $|U_i| = N_\infty(\epsilon_i, \cF_i)$. By definition, for all $f_i$ in $\cF_i$, there exists a $u_i \in U_i$ such that $\max_{\norm{x} \leq 1} \norm{f_i(x) - u_i(x)}_2 \leq \epsilon_i$. Take $U = U_r \circ U_{r-1} \circ \cdots \circ U_1 = \{u_r \circ u_{r-1} \circ \cdots \circ u_1 \}$ as the cover for $m \circ \cF$. Suppose we were given $f = f_r \circ ... \circ f_1 \in \cF$. Let $u_r, ..., u_1$ be the nearest neighbor of $f_r, ..., f_1$. Then
\begin{equation}
|m_f(x, y) - m_u(x, y)| \leq \sqrt{\sum_{i=1}^r \max_{||x|| \leq 1} ||f_i(x) - u_i(x)||_2^2}.
\end{equation}
Because $f$ and $u$ are close by construction, this is $\leq \sqrt{\sum_{i=1}^r \epsilon_i^2}$.

Having established the validity of our cover, we now return to our claim of 1-Lipschitz-ness in step 1 of this proof. We will show an upper bound for $m_{f'}(x, y) - m_f(x, y)$; the lower bound follows by symmetry.

Let $\delta_1^*, ..., \delta_r^*$ be the optimal choices of $\delta$ in defining $m_f(x, y)$. Our goal is to turn these into $\hat{\delta}_1, ..., \hat{\delta}_r$, a feasible solution of $m_{f'}(x, y)$. $m_{f'}(x, y) \leq \sqrt{\sigma \norm{\hat{\delta}_i}_2} \leftrightarrow \sqrt{\sigma||\delta_i||^2}$; the solution must be feasible for this bound on $m_{f'}(x, y)$ to hold.

We want to make $(f_1', \hat{\delta}_1, ..., \hat{\delta}_r$ behave the same way under perturbations as $(f_1, \delta_1^*, ..., \delta_r^*$. $f$ has parameters $W_1, ..., W_r$ and $f'$ has parameters $W_1', ..., W_r'$. Then,
\begin{align}
    h_1 &= W_1 x + \delta_1^* \norm{x}_2 \\
    h_2 &= \sigma(W_2 h_1) + \delta_2^* \norm{h_1}_2 \\
    &\vdots \\
    h_r &= \sigma(W_r h_{r - 1}) + \delta_r^* \norm{h_{r - 1}}_2
\end{align}
We want to imitate this by perturbing $f'$ in some way. In particular, let
\begin{equation}
    h_1 = W_1'x + \delta_1^* \norm{x}_2 + (W_1 - W_1')x,
\end{equation}
where the last term serves to compensate for the difference between $W_1$ and $W_1'$. Therefore $\hat{\delta}_1^* \defeq \delta_1^* + \frac{(W_1 - W_1')x}{\norm{x}_2}$.
We repeat this for every layer, e.g.,
\begin{equation}
    h_2 = \sigma(W_2' h_1) + \delta_2^*\norm{h_1} + \sigma(W_2 h_1) - \sigma(W_2' h_1),
\end{equation}
and $\delta_2 \cdot \norm{h} = h_2 - \sigma(W_2' h_1)$. So $\hat{\delta_2} = \delta_2^* + \frac{\sigma(w_2 h_1) - \sigma(w_2' h_1)}{\norm{h_1}_2}$. In general, 
\begin{align}
    \hat{\delta}_i = \delta_i^* + \frac{\sigma(W_ih_{i-1}) - \sigma(W_i' h_{i-1})}{\norm{h_{i-1}}_2}
\end{align} 

Then $\hat{\delta}_1, ..., \hat{\delta}_r$ on $f'$ are making the same predictions as $\delta_1, ..., \delta_r$ on $f'$. Next, we prove that this is a feasible solution.
\begin{align}
    m_{f'}(x, y) &\leq \sqrt{\sum ||\hat{\delta}_i||_2^2} \\
    &\leq \sqrt{\sum \norm{\delta_i}_2^2} + \sqrt{\sum \frac{\sigma(W_i h_{i-1}) - \sigma(W_i' h_{i-1})}{\norm{h_{i-1}}_2}}. 
\end{align} 
The last step follows by Minkowski's inequality, which states that $\sqrt{\sum \norm{a_i + b_i}_2^2} \leq \sqrt{\sum \norm{a_i}_2^2} + \sqrt{\sum \norm{b_i}_2^2}$. In this setting, this inequality can also be proved using Cauchy-Schwarz.

But $\sqrt{\sum \norm{\delta_i}_2^2}$ can be bounded above by $m_f(x, y) + \sqrt{\sum_{i=1}^r \max_{\norm{x} \leq 1} (\sigma(Wx)-\sigma(W'x))^2}$; dividing by the 2-norm here is the same as restricting the 2-norm to be 1. This equals $m_f(x, y) + \sqrt{\sum_{i=1}^r \max_{\norm{x} \leq 1} (f_i(x)-f_i'(x))^2}$, which is what we wanted to show.
\end{proof}

\tnote{maybe introduce some remarks below to make the paragraphs below a bit more structured}
We can compare the above with \cite{bartlett2017},
\begin{equation}
f(x, \delta) - f(x) \leq \norm{\delta_r} \cdot \norm{W_{r-1}}_{\text{op}} \cdots \norm{W_1}_{\text{op}} + \norm{W_r}_{\text{op}} \cdot \norm{\delta_{r-1}} \cdots \norm{W_1}_{\text{op}} + \cdots + \norm{W_r}_{\text{op}} \cdots \norm{W_2}_{\text{op}} \cdot \norm{\delta_1}.
\end{equation}
Ignoring minor details (e.g. dependency on $r$), and supposing that $y = 1$, if we want $f(x) > 0$ and $f(x + \delta) \leq 0$, then $\norm{\delta}$ must exceed $\frac{1}{\prod\norm{W_i}_{\text{op}}}$ in order to make up the difference. This says that, approximately, $\frac{m_f(x, y)}{y f(x)} \geq \frac{1}{\prod \norm{W_i}_{\text{op}}}$, so $\frac{1}{m_f(x, y)}$, the inverse margin, is $\frac{1}{yf(x)} \cdot \prod \norm{W_i}_{\text{op}}$. So, we conclude that the former is a tighter bound.

How \textit{much} better is our new bound? Empirically, it seems to be substantially better, since our empirical Lipschitzness is better than the worst-case bound. Another source of hope that this is better is that SGD prefers Lipschitz solutions and Lipschitzness on data points. The algorithm is (in a sense) minimizing Lipschitzness on a data point, making it better than the worst-case Lipschitzness on the entire domain, which likely accounts for the gap between the two bounds. (Implicitly, we are maximizing the all-layer margin.)

Moreover, SAM (sharpness-aware regularization) is a form of perturbation, but on the parameter $\theta$ itself rather than on the intermediate hidden parameters $h_i$. However, these two methods are related! If we consider the (single-example) loss $\frac{\partial \ell}{\partial W_i}$, it equals $\frac{\partial \ell}{\partial h_{i+1}} \cdot h_i^T$. Therefore the norm of the term on the left equals the product of the norms of the two terms of the right, relating the Lipschitzness of the parameters.

Finally, we can prove a more general version in which we do not need to worry about the minimum margin of the entire dataset, and instead find an average. We can show that the test error is bounded above by $\frac{1}{n}$ times $\sqrt{\frac{1}{n} \sum_{i=1}^n \frac{1}{m_f(x\sp{i}, y\sp{i})^2}}$ times the sum of complexities of each layer, plus a lower-order term.

