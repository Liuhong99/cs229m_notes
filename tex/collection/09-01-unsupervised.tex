\metadata{18}{Haoran Xu and Lewis Liu}{Nov 17th, 2021}

We venture into unsupervised learning by first studying classical (and analytically tractable) approaches to unsupervised learning. Classical unsupervised learning usually consists of specifying a latent variable model and fitting using the expectation-maximization (EM) algorithm. However, so far we do not have a comprehensive theoretical analysis for the convergence of EM algorithms because fundamentally analyzing EM algorithms involves understanding non-convex optimization. Most analysis of EM only applies to special cases (e.g., see ~\citet{xu2016global,daskalakis2016ten}) and it is not clear whether any of the results can be extended to more realistic, complex setups, without a fundamentally new technique for understanding nonconvex optimization. 

Instead, we will analyze a family of algorithms which are broadly referred to as spectral methods or tensor methods, which are a particular application of the method of moments~\citep{pearson1894} with the algorithmic technique of tensor decomposition~\citep{anandkumar2015learning}. While the spectral method appears to be not as empirically sample-efficient as EM, it has provable guarantees and arguably is more reliable than EM given the provable guarantees.

After discussing the basics of classical unsupervised learning, we will move on to modern applications of deep learning such as self-training and contrastive learning. Here we note that so-called ``semi-supervised'' and domain adaptation approaches in deep learning can often be reduced to unsupervised learning problems, so in some sense, our analysis here is related to many fields in deep learning that are not always referred to as unsupervised. 

\sec{Method of Moments}

We begin by formally describing the unsupervised learning problem. First, assume that we are studying a family of distributions $P_{\theta}$ parameterized by $\theta \in \Theta$, where $P_{\theta}$ can be described by a latent variable model. Then, given data $x^{(i)},...,x^{(n)}$ that is sampled i.i.d. from some distribution in $\{P_\theta\}_{\theta \in \Theta}$, our goal is to recover the true $\theta$. 

Perhaps the most well-studied latent variable model in machine learning is the mixture of Gaussians. We consider the following model for the mixture of $k$ $d$-dimensional Gaussians. Let 
\begin{align}
\theta = \l ( (\mu_1, \cdots, \mu_k), (p_1, \cdots, p_k)\r ),
\end{align}
where $\mu_i\in \R^d$ is the mean of the $i$-th component and $p$ is a vector of probabilities belonging to the $k$-simplex, which represents the mixture coefficient for clusters. Formally, for $\Delta(k) \defeq \{ p: \|p\|_1 = 1, p\geq 0, p\in\R^k\}$, 
\begin{align}
    p = (p_1, \cdots, p_k) \in \Delta(k).
\end{align}
We then sample $x \sim P_\theta$ in a two-step approach: 
\begin{align}
    i &\sim \text{categorical}(p), \notag\\
    x &\sim \cN(\mu_i, I).
\end{align}
Here $i$ is called the latent variable since we only observe $x$. Here we assume the covariances of the Gaussians to be identity, but they can also be parameters that are to be learned.

There are many other latent variables that could be defined via a similar generative process, such as Hidden Markov Models, Independent Component Analysis, which we will discuss later. %, and Expectation-Maximization, but here we focus on the so-called Moment Method.

\subsec{Warm-up: mixture of two Gaussians}
We first study a simple case: the mixture of two Gaussians.
In this case, $k=2$, and we assume $p_1=p_2=\frac{1}{2}$. For simplicity, we also assume $\mu_1=-\mu_2$, that is, the means of the two Gaussians are symmetric around the origin. To simplify our notation, let $\mu_1=\mu$ and $\mu_2=-\mu$. These assumptions yield the following model for $x$:
\begin{equation}
    x \sim \frac{1}{2}\mathcal{N}(\mu,I) + \frac{1}{2}\mathcal{N}(-\mu,I).
\end{equation}
To implement the moment method, we need to complete the following two tasks:
\begin{enumerate}
    \item Estimate the moment(s) of $x$ using empirical samples.
    \item Recover parameters from the moment(s) of $x$.
\end{enumerate}

The first moment of $x$ is
\begin{align}
    M_1 &\defeq \Exp [x] \\
    &= \frac{1}{2}\Exp [x|i=1]+\frac{1}{2}\Exp[x|i=2] \\
    &= \frac{1}{2}\mu + \frac{1}{2}(-\mu) \\
    &= 0.
\end{align}
Therefore, the first moment provides no information about $\mu$. We compute the second moment as
\begin{align}
    M_2 &\defeq \Exp[xx^\top] \\
    &= \frac{1}{2}\Exp[xx^\top|i=1]+\frac{1}{2}\Exp[xx^\top|i=2]
\end{align}
To compute these expectations, consider an arbitrary $Z \sim \cN(\mu, I)$. Then,
\begin{align}
    \Exp [ZZ^\top] &= \Exp[Z] \Exp[Z]^\top + \Cov(Z) \\
    &= \mu \mu^\top + I
\end{align}
Recognizing that this second moment calculation is the same for both Gaussians in our mixture, we obtain:
\begin{align}
    M_2 &= \frac{1}{2}(\mu\mu^\top+I)+\frac{1}{2}(\mu\mu^\top+I) \\
    &=\mu\mu^\top+I
\end{align}
Since the second moment provides information about $\mu$, we can complete the two tasks required for the moment method using the second moment.

If we had access to infinite data, then we can compute the exact second moment $M_2=\mu\mu^\top+I$. Then, we can recover $\mu$ by evaluating the top eigenvector and eigenvalue of $M_2$.\footnote{This approach is known as the spectral method.} The top eigenvector and eigenvalue of $M_2$ is $\bar{\mu} \defeq \frac{\mu}{\norm{\mu}_2}$ and $\norm{\mu}_2^2+1$, respectively. 

In practice, however, we do not have infinite data. In that case, we need to estimate the second moment by an empirical average.
\begin{align}
    \widehat{M}_2=\frac{1}{n}\sum_{i=1}^nx\sp{i} {x\sp{i}}^\top
\end{align}
We can then recover $\mu$ by evaluating the top egivenvector and eigenvalue of $\widehat{M}_2$. However, we need this algorithm to be robust to errors, i.e., similar estimates, $\widehat{M}_2$, of the second moment should yield similar estimates of $\mu$. Fortunately, most algorithms we might use for obtaining the top eigenvector and eigenvalue are robust, so we can limit our attention to the infinite data case. Having outlined the moment method approach to the mixture of two Gaussians problem, we study a generalization of this problem in the sequel.

\subsec{Mixture of Gaussians with more components}

The general moment method for solving latent variable models is summarized by the following steps.
\begin{enumerate}
    \item Compute $M_1=\Exp[x]$, $M_2=\Exp[xx^\top]$, $M_3=\Exp[x\otimes x\otimes x],$ $M_4 = \cdots$. Note that $x\otimes x\otimes x$ is in $\mathbb{R}^{d\times d\times d}$ and $(x\otimes x\otimes x)_{ijk}=x_i\cdot x_j\cdot x_k$. For example, $M_{3,ijk}=\Exp[x_ix_jx_k]$.
    \item Design as algorithm $A(M_1, M_2, M_3,\dots)$ that outputs $\theta$.
    \item Show that $A$ is robust to errors in our moment estimates, i.e., we apply $A$ to $(\widehat{M}_1,\widehat{M}_2,\widehat{M}_3,...)$ in reality.
\end{enumerate}
In the sequel, we instantiate this paradigm for mixtures of $k$ Gaussians ($k\geq 3$). 

For the simplicity of demonstrating the idea, we assume $p_1 = \cdots = p_k =\frac{1}{k}$, i.e. $i \stackrel{\text{unif}} \sim[k]$, and $x\sim\mathcal{N}(\mu_i,I)$. Equivalently, 
\begin{equation}
    x\sim\frac{1}{k}\sum_{i=1}^k\mathcal{N}(\mu_i,I).
\end{equation}
In this example, we only describe steps (1) and (2) in the general algorithm described above.  

We first evaluate the first and second moments. The first moment follows from
\begin{align}
    M_1 &=\Exp[x] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[x|i] \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i,
\end{align}
and the second moment is computed as
\begin{align}
    M_2 &= \Exp[xx^\top] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[xx^\top|i] \\
    &=\sum_{i=1}^k\frac{1}{k}(\mu_i\mu_i^\top+I) \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top + I.
\end{align}

\subsubsection{Second moments do not suffice}
Can we recover $\mu=(\mu_1,...,\mu_k)$ from $\frac{1}{k}\sum_{i=1}^k\mu_i$ and $\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top$? Unfortunately, in most of the cases when $k\geq 3$, the first and second moments are not sufficent to recover $\mu$. 

One reason is the so-called ``missing rotation information'' problem. Let 
\begin{equation}
    U =\begin{bmatrix} \vrule & & \vrule \\ \mu_1 & \cdots & \mu_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
\end{equation}
denote the matrix we aim to recover. Then, consider some rotation matrix $R\in\mathbb{R}^{k\times k}$. We consider $U$ versus $U R$:
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top &= \frac{1}{k}U U^\top \\
    &=\frac{1}{k}(U R)(U R)^\top &\text{($RR^\top=I$)}
\end{align}
This result proves that the second moment is invariant to rotations. To prove a similar claim for the first moment, we also constrain our choice of $R$ such that
\begin{align}
    R\cdot\Vec{1}=\Vec{1}
\end{align}
Then,
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i&=\frac{1}{k} U \cdot\Vec{1} \\
    &=\frac{1}{k} U R\cdot\Vec{1}
\end{align}
Therefore, the first and second moments of $U$ and $U R$ are indistinguishable, and we must consider the third moment in order to identify $U$.

\subsubsection{Computing the third moment}

The third moment is the tensor $\Exp[x \otimes x \otimes x] \in \mathbb{R}^{d \times d \times d}$. To express this expectation in terms of tractable quantities, we can condition on the Gaussian observed and average:
\begin{align}
	\Exp[x \otimes x \otimes x] = \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i]
\end{align}

Each term in the sum now corresponds to the third moment for some multivariate Gaussian. Fortunately, Lemma~\ref{lec19:lma:gaussian_third_moment} suggests a formula for estimating its value.
\begin{lemma} \label{lec19:lma:gaussian_third_moment}
Suppose $z \in \cN(v, I)$. Then, 
\begin{align}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d \Exp[z] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[z] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[z] 
\end{align}
where $e_1,\dots,e_d$ denote the canonical basis vectors.
\end{lemma}
Note that $\Exp[z] = v$, which means that we can compute $v \otimes v \otimes v$ from a linear combination of $\Exp[z \otimes z \otimes z]$  and $\Exp[z]$.

\begin{proof}
We compute the third moment element-wise. That is,
\begin{align}
	(\Exp[z \otimes z \otimes z])_{ijk} &= \Exp[z_i  z_j z_k] \\
	&= \Exp[(v_i + \xi_i)\cdot (v_j + \xi_j) \cdot (v_k + \xi_k)]  &\text{$(z = v + \xi, \xi \sim \cN(0, I))$}\\
	&= v_i v_j v_k + \underbrace{\Exp [v_i v_j \xi_k] +  \Exp [v_i \xi_j v_k] +  \Exp [\xi_i v_j v_k]}_{=0} \nonumber \\ 
	&\quad + \Exp[v_i \xi_j \xi_k] + \Exp[v_j \xi_i \xi_k] + \Exp[v_k \xi_i \xi_j] + \Exp[\xi_i \xi_j \xi_k] \label{lec19:eqn:higher_moments} 
\end{align}
To explicitly compute the last four terms in \eqref{lec19:eqn:higher_moments}, we note that:
\begin{align}
    \Exp[\xi_i \xi_k] = \begin{cases} 0 &\text{if $i \neq k$} \\ \Exp[\xi_i^2] = 1 &\text{if $i = k$} \end{cases}
\end{align} 
and that for any choice of $i, j,$ and $k$,
\begin{align}
    \Exp[\xi_i \xi_j \xi_k] = 0.
\end{align}
Therefore, 
\begin{equation}
	(\Exp[z \otimes z \otimes z])_{ijk} = v_i v_j v_k + v_i \ind{j=k} +  v_j \ind{i=k}  +  v_k \ind{i=j} 
\end{equation}

Since $(\sum_{l=1}^d v \otimes e_l \otimes e_l)_{ijk} = \sum_{l=1}^d v_i  e_{lj}  e_{lk} = v_i \bbI(j=k)$, we have proven that:
\begin{equation}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d v \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes v \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes v.
\end{equation} 
\end{proof}

We can now apply Lemma~\ref{lec19:lma:gaussian_third_moment} to compute the third moment of the mixture of $k$ Gaussians. In particular,
\begin{align}
	\Exp[x \otimes x \otimes x] & =  \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i] \\
	&=  \frac{1}{k} \sum_{i=1}^k \l (\mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \mu_i \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \mu_i \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \mu_i \r ) \\
	&=  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \nonumber \\
    &\quad + \sum_{l=1}^d  e_l \otimes e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r) \\
	& =  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \Exp[x] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[x] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[x]\\
\end{align} 

For notational convenience, let
\begin{equation}
    a^{\otimes 3} \defeq a \otimes a \otimes a.
\end{equation} 
So far, we have shown how to compute $\frac{1}{k} \sum_{i=1}^k \mu_i^{\otimes 3}$ from $\Exp[x^{\otimes 3}]$ and $\Exp[x]$. In the sequel, we show how to estimate the latent variable by describing several approaches one might take to recover $\{\mu_i\}_{i = 1}^k$ from $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$. 

\paragraph{Tensor decomposition}
Recovering the Gaussian means, $\{\mu_i\}_{i = 1}^k$,  from the third mixture moment, $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$, is a special case of the general tensor decomposition problem. That problem is set up as follows: Assume that $a_1, \cdots a_k \in \bbR^d$ are unknown. Then, given $\sum_{i=1}^k a_i^{\otimes 3}$, our goal is to reconstruct the $a_i$ vectors. 

Before we present some standard results on tensor decomposition, we first describe some basic facts about tensors. Much like matrices, tensors have an associated rank. For example, $a \otimes b \otimes c$ is a rank-1 tensor. In general, the rank of a tensor $T$ is the minimum $k$ such that $T$ can be decomposed as 
\begin{equation}
    T = \sum_{i=1}^k a_i \otimes b_i \otimes c_i.
\end{equation} 
for some $\{a_i\}_{i =1}^k, \{b_i\}_{i =1}^k, \{c_i\}_{i =1}^k$.
Another difference between tensors and matrices is that the former objects do not have the typical rotational invariance. In particular, there is no tensor analogue to the standard matrix result that 
\begin{equation}
    AA^\top = A RR^\top A^\top.
\end{equation}
However, tensors do maintain an interesting (and useful) permutation invariance; that is, $T = \sum_{i = 1}^k a_i^{\otimes 3}$ is invariant to permutations of the indices of $a_i$.

We now summarize some standard results regarding tensor decomposition for $T = \sum_{i = 1}^k a_i^{\times 3}$. We will not prove these results here.
\begin{enumerate}
\setcounter{enumi}{-1}
\item  In the most general case, recovering the $a_i$'s from $T$ is computationally hard. Any procedure will either fail to find a unique $a_i$ or it fails to find $a_i$ \emph{efficiently}. 
\item In the orthogonal case, i.e. $a_1,\dots,a_k$ are orthogonal vectors, each $a_i$ is a global maximizer of 
\begin{equation}
    \max_{\|x\|_2 = 1} T(x,x,x) = \sum_{i,j,k} T_{ijk} x_i x_j x_k
\end{equation}
We can heuristically think of $a_i$ as eigenvectors of $T$ and there exists an algorithm to compute $a_i$ in poly-time.
\item In the independent case, i.e. $a_1,\dots,a_k$ are linearly independent. Jennrich's algorithm can be used to efficiently recover $\{a_i\}_{i = 1}^k$.
\end{enumerate} 
Results 1 and 2 above both involve the so-called ``under-complete'' case ($k \leq d$), e.g., when the number of Gaussians in the mixture is smaller than the dimension of the data. Next, we describe certain overcomplete cases for which efficient tensor decomposition is possible.
\begin{enumerate}
\setcounter{enumi}{2}
\item Suppose $a_1^{\otimes2},\dots,a_k^{\otimes2}$ are independent for $k \leq d^2$. Then, applying Result 2, we can recover $a_i$ from $\sum_{i=1}^k (a_i^{\otimes2})^{\otimes3} = \sum_{i=1}^k (a_i^{\otimes6}) \in \bbR^{d^6}$.
\item Excluding an algebraic set of measure $0$, we can use the FOOBI algorithm to recover $a_i$ from the fourth-order tensor $\sum_{i = 1}^k a_i^{\otimes 4}$ when $k \leq d^2$. A robust version of the FOOBI algorithm is described in \citet{ma2016poly}.
\item Assume $a_i$ are \emph{randomly} generated unit vectors. Then, for the third order tensor, $k$ can be large as $d^{1.5}$ \cite{ma2016poly, schrammsteurer17}. 
\end{enumerate}

In summary, the moment method is a recipe in which we first compute high order moments (i.e. tensors), assume that these estimates are noiseless, and decompose these tensors to recover the latent variables. Though we do not discuss these results here, there is an extensive literature analyzing the robustness of the moment method to error in the moment estimates. Last, we note that even though we only explicitly analyze the mixture of Gaussians model here, latent variable models amenable to analysis by the moment method include ICA, Hidden Markov Models, topic models, etc.

\sec{Spectral Clustering}
Introduced by \citet{shi2000normalized} and \citet{ng2002spectral}, spectral clustering learns a model for the data points using a \emph{pairwise} notion of similarity. Formally, assume that we are given $n$ data points $x\sp{1}, \dots, x\sp{n}$ as well as a similarity matrix $G \in \bbR^{n\times n}$ such that 
\begin{equation}
    G_{ij} = \rho (x\sp{i}, x\sp{j})
\end{equation}
where $\rho$ is some measure of similarity that assigns larger values to more similar pairs of points. 

For example, $x\sp{i}$ could denote images for which $\rho (x\sp{i}, x\sp{j})$ measures the semantic similarity. Alternatively, $x\sp{i}$ might be users of a social network and $\rho (x\sp{i}, x\sp{j}) = 1$ if $x\sp{i}$ and $x\sp{j}$ are friends. 

Our goal is to cluster the data points by viewing $G$ as a graph. For instance, in the social network example, we can naturally view $G$ as an \emph{unweighted} graph, where $G_{ij} \in \{0, 1\}$ defines the edges. Then, the clustering problem is to partition the graph into distinct neighborhoods. As we will see repeatedly in the sequel, the eigendecomposition of $G$ is closely related to this graph paritioning problem.
\subsec{Stochastic block model} 
In the stochastic block model (SBM), $G$ is assumed to be generated randomly from two hidden communities. Formally, 
\begin{equation}
    \{ 1, \cdots n \} = S \cup \bar{S},
\end{equation}
where $S$ and $\bar{S}$ partition $[n]$. Assume $|S| = \frac{n}{2}$. We then assume the following generative model for $G$. 
If $i,j \in S$ or $i,j \in \bar{S}$, then 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $p$} \\
        0 &\text{w.p. $1-p$} \end{cases}.
\end{align} 
Otherwise, for $i$ and $j$ in distinct components, 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $q$} \\
        0 &\text{w.p. $1-q$} \end{cases}
\end{align} 
for $p < q$.

Our goal is then to recover $S$ and $\bar{S}$ from $G$; the primary tool we use towards this goal is the eigendecomposition of $G$.

In some trivial cases, it is not necessary to eigendecompose $G$ to recover the two hidden communities. Suppose, for instance, that $p = 0.5$ and $q = 0$. Then, the graph represented by $G$ will contain two connected components that correspond to $S$ and $\bar{S}$.

As a warm-up to motivate our approach, we eigendecompose $\bar{G} = \Exp[G]$. Observe that
\begin{align}
    \bar{G}_{ij} = \begin{cases}
        p &\text{if $i,j$ from the same community} \\
        q &\text{o.w.} \end{cases}.
\end{align}
It is then easy to see that $\bar{G}$ is a matrix of rank $2$:
\begin{align}
    \bar{G} = \left[
        \begin{array}{c|c}
        p \cdots p & q \cdots q \\
        \vdots & \vdots \\
        p \cdots p & q \cdots q\\
        \hline
        q \cdots q & p \cdots p \\
        \vdots & \vdots \\
        q \cdots q & p \cdots p 
        \end{array}
        \right].
\end{align}
\begin{lemma} \label{lec19:lma:sbm_eigen}
Let $\bar{G} = \Exp[G]$ for the stochastic block model. Then, letting $v_i(A)$ denote the $i$-th eigenvector of the matrix $A$,
\begin{align}
    v_1(\bar{G}) &= \vec{1} \label{lec19:eqn:top_eig_G}\\
    v_2(\bar{G}) &= [\underbrace{1, \dots, 1}_{|S|}, \underbrace{-1, \dots, -1}_{|\bar{S}|}]^\top \label{lec19:eqn:second_eig_G}
\end{align}
where $v_2(\bar{G})$ has $|S|$ entries of $1$ and $|\bar{S}|$ entries of $-1$.
\end{lemma}

\begin{proof}
We begin by directly proving \eqref{lec19:eqn:top_eig_G}.
\begin{align}	
	\bar{G} \cdot \vec{1}  &= \begin{bmatrix}
           \frac{pn}{2} + \frac{qn}{2} \\
           \vdots \\
           \frac{pn}{2} + \frac{qn}{2}
         \end{bmatrix} \\
         &= \frac{p+q}{2} \cdot n \cdot \vec{1}.
\end{align}
More generally, $\vec{1}$ is the top eigenvector for any matrix with fixed row sum or any graph with uniform degree. 

Next, we prove \eqref{lec19:eqn:second_eig_G}. Let 
\begin{align}
    G' = \left[
        \begin{array}{c|c}
        r \cdots r \\
        \vdots & \makebox{\text{\huge 0}} \\
        r \cdots r \\
        \hline
        & r \cdots r \\
        \makebox{\text{\huge 0}} & \vdots \\
        & r \cdots r
        \end{array}
        \right]
\end{align}
for $r = p - q$. To precisely define $G'$, we note that $G'$ is block diagonal with two blocks of size $|S|$ and $|\bar{S}|$, respectively. Then, 
\begin{align}	
	\bar{G} &= \vec{1} \vec{1}^\top q + G'. \label{lec19:eqn:barG}
\end{align}
Thus,
\begin{align}
 G' \cdot u &= \left[
    \begin{array}{c|c}
    r \cdots r \\
    \vdots & \makebox{\text{\huge 0}} \\
    r \cdots r \\
    \hline
    & r \cdots r \\
    \makebox{\text{\huge 0}} & \vdots \\
    & r \cdots r
    \end{array}
    \right] \cdot \begin{bmatrix}
           1 \\ \vdots \\ 1\\ -1 \\
           \vdots \\
           -1
         \end{bmatrix} = r \cdot \frac{n}{2} \cdot u. \label{lec19:eqn:gprimeu}
\end{align}
Then, because $u \perp \vec{1}$, we can combine \eqref{lec19:eqn:barG} and \eqref{lec19:eqn:gprimeu} to obtain
\begin{align}
\bar{G} \cdot u =  G' \cdot u =  r \cdot \frac{n}{2} \cdot u  = \frac{p-q}{2} \cdot n \cdot u
\end{align}
Thus, $u$ has eigenvalue $\frac{p-q}{2} \cdot n$. 
\end{proof}

\begin{remark}
    Lemma~\ref{lec19:lma:sbm_eigen} shows that
    \begin{equation}
        \bar{G} = \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top.
    \end{equation}
\end{remark}
More generally, when we have more than two clusters in the graph, $G'$ is block diagonal with more than two blocks. In this setting, the eigenvectors will still align with the blocks. We illustrate this below for a generic block diagonal matrix. Let 
\begin{align}
    A = \left[
        \begin{array}{c|c|c}
        1 \cdots 1 &&\\
        \vdots & \makebox{\text{\huge 0}} & \makebox{\text{\huge 0}} \\
        1 \cdots 1 &&\\
        \hline
        & 1 \cdots 1 \\
        \makebox{\text{\huge 0}} & \vdots & \makebox{\text{\huge 0}} \\
        & 1 \cdots 1 \\
        \hline
        & & 1 \cdots 1 \\
        \makebox{\text{\huge 0}}& \makebox{\text{\huge 0}} & \vdots \\
        & & 1 \cdots 1
        \end{array}
        \right]
\end{align}

Then, the three eigenvectors of $A$ are 
\begin{align}
    \begin{bmatrix}
        1 \\ \vdots \\ 1\\ 0 \\
        \vdots \\
        0 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 1 \\
        \vdots \\
        1 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 0 \\
        \vdots \\
        0 \\ 1 \\ \vdots \\ 1
      \end{bmatrix} \label{lec19:eqn:eigenmatrix}
\end{align}
Furthermore, the rows of the matrix formed by the three eigenvectors given by \eqref{lec19:eqn:eigenmatrix} clearly give the cluster IDs of the vertices in $G$. Note also that permutations of $A$ will result in equivalent permutations in the coordinates of each of the three eigenvectors.

Next, we relate this observation to the result in Lemma~\ref{lec19:lma:sbm_eigen}. While there are no negative values in the eigenvectors given in \eqref{lec19:eqn:eigenmatrix}, we observe that any linear combination of eigenvectors is also an eigenvector, so recovering a solution that look more like \eqref{lec19:eqn:second_eig_G} is straightforward. Indeed, taking linear combinations of the eigenvectors defined above shows that there is an alternative eigenbasis that includes the all-ones vector, $\vec{1}$. How for this choice of $A$, the all-ones vector is not the unique top eigenvector. For that to be the case, we require background noise in $\bar{G}$.

In reality, we only observe $G$. In the sequel, we will show that in terms of the spectrum, $G \approx \Exp[G]$. Formally, we will leverage earlier concentration results to prove that $\norm{G - \Exp[G]}_{\text{op}}$ is small. Concretely, then,
\begin{align}
    G &= (G - \Exp[G]) + \Exp[G] \\
    &= (G - \Exp[G]) + \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top
\end{align}
Rearranging, we obtain that:
\begin{align}
    G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top &= (G - \Exp[G]) + \frac{p - q}{2} \cdot u u^\top
\end{align}
We then hope that $G - \Exp[G]$ is a small perturbation, so that the top eigenvector of $G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top$ is close to $u$. Namely, it suffices to show that 
\begin{equation}
    \norm{G - \Exp[G]}_{\text{op}} \ll \l \|\frac{p - q}{2} \cdot uu^\top\r \|_{\text{op}}.
\end{equation}